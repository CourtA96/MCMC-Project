{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perlmutter99 Data Analysis\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In their paper entitled \"Measurements of Omega and Lambda from high-redshift supernovae\", S. Perlmutter et al used data from the Supernova Cosmology Project to calculate the matter density of the universe, Omega_matter. The data gives the luminosity (m) for different amounts of redshift (z), and m is a function of Omat, among other parameters. The value they calculated was approximately 0.28(+/-)1. This project uses the data from their paper to calculate Omega_matter using Markov Chain Monte Carlo (MCMC) methods.\n",
    "\n",
    "MCMC methods are used to take samples of a probability distribution. In this project the probabilty distribution is the log-likelihood in the foreground-background model, where the foreground and background distributions are given by gaussians. The MCMC sampler takes in initial parameters and alters them, travelling though the space of possible parameters and accepting or rejecting parameters based on the log-likelihood.\n",
    "\n",
    "This project uses both the Gibbs sampler and the EMCEE sampler, which are predicted to yeild results that are similar to eachother as a check of both methods.\n",
    "\n",
    "## The Code\n",
    "\n",
    "### Setting up the function to fit\n",
    "\n",
    "The following is code written by Dr. Dustin Lang. It creates the function that is being fitted. Comments by Dr. Lang are in italics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Pkg\n",
    "# This is a numerical integration package that we use for cosmology distances\n",
    "#Pkg.add(\"QuadGK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Flat Lambda CDM, the distances should be computable using a \"special\" function\n",
    "# hyp2f1 which is in scipy but I couldn't find in Julia other than in the \"Nemo\" package,\n",
    "# but I couldn't get that working, so below I'm just using the integral version.\n",
    "# Pkg.add(\"Nemo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "using QuadGK\n",
    "#using Nemo\n",
    "using Plots\n",
    "using Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is a cheap function to read in the data table.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "function read_data()\n",
    "    open(\"p99-data.txt\", \"r\") do io\n",
    "        line = readline(io)\n",
    "        # drop \"#\"\n",
    "        if line[1] == '#'\n",
    "            line = line[2:end]\n",
    "        end\n",
    "        words = split(line)\n",
    "        # println(words)\n",
    "        ncols = length(words)\n",
    "        lines = readlines(io)\n",
    "        nlines = length(lines)\n",
    "        #println(\"Read \", nlines, \" lines\")\n",
    "        names = []\n",
    "        redshift = zeros(nlines)\n",
    "        m_b_eff = zeros(nlines)\n",
    "        sigma_m_b_eff = zeros(nlines)\n",
    "        for i in 1:nlines\n",
    "            #println(\"Line: \", lines[i])\n",
    "            # HACK --- I'm hard-coding which columns contain which data\n",
    "            words = split(lines[i])\n",
    "            push!(names, words[1])\n",
    "            redshift[i] = parse(Float64, words[2])\n",
    "            m_b_eff[i] = parse(Float64, words[9])\n",
    "            sigma_m_b_eff[i] = parse(Float64, words[10])\n",
    "        end\n",
    "        return names, redshift, m_b_eff, sigma_m_b_eff\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and plot the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(name, z, m, merr) = read_data();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(z, m, yerr=merr, seriestype=:scatter, label=\"\", xlabel=\"Redshift z\", ylabel=\"m_B\",\n",
    "title=\"Perlmutter+99 Supernovae\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Distance functions for a Flat Lambda CDM cosmology model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These functions are taken from astropy.cosmology, specialized for\n",
    "# the FlatLambdaCDM model.\n",
    "#\n",
    "\n",
    "# A minimal Flat Lambda CDM model\n",
    "mutable struct LCDM\n",
    "    Om0::Float64\n",
    "    Ode0::Float64\n",
    "    hubble_distance::Float64\n",
    "end\n",
    "function luminosity_distance(cosmology, z)\n",
    "    return (1. + z) * comoving_transverse_distance(cosmology, z)\n",
    "end\n",
    "function comoving_transverse_distance(cosmology, z)\n",
    "    return comoving_transverse_distance_z1z2(cosmology, 0., z)\n",
    "end\n",
    "function comoving_transverse_distance_z1z2(cosmology, z1, z2)\n",
    "    # HERE we're assuming Ok0=0\n",
    "    # (Omega curvature; the effective curvature density/critical density at z=0)\n",
    "    return comoving_distance_z1z2(cosmology, z1, z2)\n",
    "end\n",
    "\n",
    "function comoving_distance_z1z2(cosmology, z1, z2)\n",
    "    #hypergeometric_comoving_distance_z1z2(cosmology, z1, z2)\n",
    "    integral_comoving_distance_z1z2(cosmology, z1, z2)\n",
    "end\n",
    "\n",
    "function integral_comoving_distance_z1z2(cosmology, z1, z2)\n",
    "    function flcdm_inv_efunc_norel(z)\n",
    "        Om0 = cosmology.Om0\n",
    "        Ode0 = cosmology.Ode0\n",
    "        return ((1. + z)^3 * Om0 + Ode0) ^ -0.5\n",
    "    end\n",
    "    # HERE we turn off relativistic species (Tcmb=0)\n",
    "    (integral,error) = quadgk(flcdm_inv_efunc_norel, z1, z2)\n",
    "    return cosmology.hubble_distance * integral\n",
    "end\n",
    "\n",
    "function hypergeometric_comoving_distance_z1z2(cosmology, z1, z2)\n",
    "    # def _hypergeometric_comoving_distance_z1z2(self, z1, z2):\n",
    "    s = ((1 - cosmology.Om0) / cosmology.Om0) ^ (1. / 3)\n",
    "    # Use np.sqrt here to handle negative s (Om0>1).\n",
    "    prefactor = cosmology.hubble_distance / sqrt(s * cosmology.Om0)\n",
    "    return prefactor * (T_hypergeometric(s / (1 + z1)) -\n",
    "                        T_hypergeometric(s / (1 + z2)))\n",
    "end\n",
    "function T_hypergeometric(x)\n",
    "    #from scipy.special import hyp2f1\n",
    "    return 2 * sqrt(x) * Nemo.hyp2f1(1. / 6, 1. / 2, 7. / 6, -x^3)\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perlmutter+ define D_L = H_0 d_L; you could drop the factor of hubble_distance\n",
    "# from luminosity_distance here if you wanted.  It will all just get folded into the\n",
    "# M_B offset.\n",
    "function distance_modulus(universe, z)\n",
    "    5. * log10.(luminosity_distance(universe, z) / 10.)\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the log-likelihood function and rewrite in a more accessible form\n",
    "\n",
    "The log-likelihood is calculated using a foreground-background model, where the foreground distribution is a gaussian centered at the predicted value of m with variance given by the error in m (merr), the background distribution is a gaussain centered at Y with standard deviation V. This code is altered from the class notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fgbg_SNLLL(x, y, sigma, M, Omat, pbad, Y, V)\n",
    "    # If the \"pbad\" parameter goes outside the range 0 to 1, the prior is zero, so bail out.\n",
    "    if (pbad < 0) || (pbad > 1)\n",
    "        return -Inf\n",
    "    end\n",
    "    # If the \"V\" parameter goes negative, the prior is zero; bail out.\n",
    "    if (V < 0 || V > 100)\n",
    "        return -Inf\n",
    "    end\n",
    "    # If the \"Omat\" parameter goes outside of (0,1), the result is unphysical; bail out.\n",
    "    if (Omat < 0 || Omat > 1)\n",
    "        return -Inf\n",
    "    end\n",
    "#    if M < 0\n",
    "#        return -Inf\n",
    "#    end\n",
    "    \n",
    "    # Use the input values to calculate the function:\n",
    "    \n",
    "    universe = LCDM(Omat ,1.0-Omat, 60.)\n",
    "    DM = map(x->distance_modulus(universe,x),x)\n",
    "\n",
    "    predM = M .+ DM\n",
    "    \n",
    "    # Calculate foreground and background probilities:\n",
    "    p_fg = @. 1. / (sqrt(2. *pi) * sigma) * exp(-0.5 * (predM - y)^2/(sigma^2))\n",
    "    bg_var = @. sigma^2 + V\n",
    "    p_bg = @. 1. / sqrt(2. *pi*bg_var) * exp(-0.5 * (y - Y)^2/bg_var)\n",
    "    \n",
    "    # Here, we weight the foreground probability by 1-pbad and the background by pbad,\n",
    "    # and then take the log.\n",
    "    \n",
    "    lnl = sum(log.((1. .- pbad).*p_fg .+ pbad.*p_bg))\n",
    "    return lnl\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simpler version of fgbg_SNLLL:\n",
    "\n",
    "function fgbg_simp(params)\n",
    "    M1, Omat1, pbad1, Y1, V1 = params\n",
    "    return fgbg_SNLLL(z_new, m_new, merr, M1, Omat1, pbad1, Y1, V1)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to the function: Gibbs Sampling\n",
    "\n",
    "Before beginning the fit, it is necessary to create a function to reorder the data points from lowest z to highest z. This is because, even though the log-likelihood function will work regardless of the order of the points, when the distance_modulus function is plotted against the values of z, the points in the function are connected in order of their position in the array. If the data is not reordered, Julia will connect the data points in the order they appear in the original array, not in order of their z-components. This is demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reorder the arrays\n",
    "function reord(a,b)\n",
    "    n = length(a)\n",
    "    ai_temp = 0\n",
    "    bi_temp = 0\n",
    "    \n",
    "    # Create new empty arrays\n",
    "    a_new = zeros(n)\n",
    "    b_new = zeros(n)\n",
    "    \n",
    "    # Fill the new arrays with the values from the old arrays\n",
    "    for p in 1:n\n",
    "        a_new[p] = a[p]\n",
    "        b_new[p] = b[p]\n",
    "    end\n",
    "    \n",
    "    # Reorder the arrays\n",
    "    for i in 1:n\n",
    "        for j in 1:n\n",
    "            if a_new[i] < a_new[j]\n",
    "                ai_temp = a_new[i]\n",
    "                bi_temp = b_new[i]\n",
    "                a_new[i] = a_new[j]\n",
    "                b_new[i] = b_new[j]\n",
    "                a_new[j] = ai_temp\n",
    "                b_new[j] = bi_temp\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Output the reordered arrays\n",
    "    return a_new,b_new\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder the data points\n",
    "z_new,m_new = reord(z,m);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate what occurs when the data is not reordered\n",
    "universe = LCDM(0.14,1-0.14, 60.)\n",
    "DM = map(z_new->distance_modulus(universe,z_new),z_new)\n",
    "DM2 = map(z->distance_modulus(universe,z),z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(z, m, yerr=merr, seriestype=:scatter, label=\"\", xlabel=\"Redshift z\", ylabel=\"m_B\",\n",
    "title=\"Perlmutter+99 Supernovae\")\n",
    "plot!(z_new,DM .+ 20.1,color=:blue)\n",
    "plot!(z,DM2 .+ 20,color=:red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above figure shows the importance of reordereing the data. The unordered plot, shown in blue, has lines that connect the different points of the function according to their position in the array. The ordered plot, in red, connects the points of the function in order of their z-values, giving a smooth curve.\n",
    "\n",
    "Now we can use a Gibbs sampling MCMC to find the line of best fit. The run_gibbs_mcmc function was written by Dr. Lang and was copied from the course notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_gibbs_mcmc(logprob_function, initial, jumpsizes, nsteps)\n",
    "    nparams = length(initial)\n",
    "    chain = zeros((nsteps, nparams))\n",
    "    logprobs = zeros(nsteps)\n",
    "    params = initial\n",
    "    logprob = logprob_function(params)\n",
    "    accepts = zeros(nparams)\n",
    "    tries = zeros(nparams)\n",
    "    for i in 1:nsteps\n",
    "        # Choose which parameter to adjust this time\n",
    "        j = rand(1:nparams)\n",
    "        # Jump just that parameter.\n",
    "        params_new = copy(params)\n",
    "        params_new[j] += randn() * jumpsizes[j]\n",
    "        logprob_new = logprob_function(params_new)\n",
    "        if (exp(logprob_new - logprob) >= rand(Float64))\n",
    "            logprob = logprob_new\n",
    "            params = params_new\n",
    "            accepts[j] += 1\n",
    "        end\n",
    "        tries[j] += 1\n",
    "        chain[i,:] .= params\n",
    "        logprobs[i] = logprob\n",
    "    end\n",
    "    return chain, logprobs, accepts ./ tries\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting to the function: EMCEE sampling\n",
    "\n",
    "We can also use EMCEE sampling to find the best fit. The ensemble_sampler code code was written by Dr. Lang and was copied from the course notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ensemble_sampler(logprob_func, initial, nsteps)\n",
    "    # A translation of the \"emcee\" ensemble sampler by Dan Foreman-Mackey,\n",
    "    # http://arxiv.org/abs/1202.3665.  This is \"Algorithm 3\" in that paper.\n",
    "    #\n",
    "    # *logprob_func* is a log-posterior-probability function of the parameters.\n",
    "    # *initial* must be an array with shape (Nwalkers, Nparams)\n",
    "    # *nsteps* is the desired number of MCMC steps to take\n",
    "    #\n",
    "    nwalkers, nparams = size(initial)\n",
    "    if (nwalkers % 2 != 0)\n",
    "        println(\"NWalkers must be even\")\n",
    "        return\n",
    "    end\n",
    "    chain = zeros((nsteps, nwalkers, nparams))\n",
    "    logprobs = zeros(nwalkers)\n",
    "    nhalf = Int(nwalkers/2)\n",
    "    # Start at initial parameter values\n",
    "    params = initial\n",
    "    for j in 1:nwalkers\n",
    "        logprobs[j] = logprob_func(params[j,:])\n",
    "    end\n",
    "    # \"Stretch move\" parameter\n",
    "    alpha = 2.\n",
    "    # Tried moves\n",
    "    tries = 0\n",
    "    # Accepted moves\n",
    "    accepts = 0\n",
    "    for i in 1:nsteps\n",
    "        # At each step, we first update one half of the ensemble (holding the other half fixed)\n",
    "        # and then updated the second half (holding the first half, with its updated values, fixed)\n",
    "        for half in 0:1\n",
    "            for j in 1:nhalf\n",
    "                # We're going to update the walker at index \"me\":\n",
    "                me = half*nhalf + j\n",
    "                # Draw a walker from the other half of the walkers\n",
    "                other = (1-half)*nhalf + rand(1:nhalf)\n",
    "                # Draw the \"stretch move\" distance\n",
    "                z = ((alpha - 1.) * rand(Float64) + 1)^2 / alpha\n",
    "                # Compute the parameter value -- it is along the line connecting \"me\" and \"other\"\n",
    "                params_new = params[other,:] .+ z .* (params[me,:] - params[other,:])\n",
    "                # Compute the log-prob at the new sample location\n",
    "                logprob_new = logprob_func(params_new)\n",
    "                # There is an extra term that weights the proposal distribution -- required to maintain\n",
    "                # detailed balance.\n",
    "                logproposal = (nparams-1) * log(z)\n",
    "                tries += 1\n",
    "                # Keep this new sample?\n",
    "                if exp(logproposal+logprob_new - logprobs[me]) >= rand(Float64)\n",
    "                    # Keep it!\n",
    "                    params[me,:] = params_new\n",
    "                    logprobs[me] = logprob_new\n",
    "                    accepts += 1\n",
    "                end\n",
    "                # Record the chosen parameter values\n",
    "                chain[i, me, :] .= params[me, :]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    return chain, accepts/tries\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First run both the Gibbs and EMCEE sampler with an initial guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the Gibbs sampler by guessing some initial parameters\n",
    "initial = [15, 1, 0.1, mean(m), var(m)]\n",
    "jumpsizes = [2, 0.2, 0.1, 2., 1.]\n",
    "nsteps1 = 100000\n",
    "chain1,logprobs1,acceptance1 = run_gibbs_mcmc(fgbg_simp, initial, jumpsizes, nsteps1);\n",
    "acceptance1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use the same initial parameters as before for the EMCEE sampler\n",
    "\n",
    "jumpsizes = ones(length(initial)) * 1e-6\n",
    "nparams = length(initial)\n",
    "\n",
    "nwalkers = 20\n",
    "\n",
    "# Initialize the walkers in a ball around *initial*.\n",
    "walkers = zeros((nwalkers, nparams))\n",
    "for i in 1:nwalkers\n",
    "    walkers[i,:] = initial + randn(nparams) .* jumpsizes\n",
    "end\n",
    "# Call the sampler\n",
    "nsteps2 = 10_000\n",
    "chain2,acceptance2 = ensemble_sampler(fgbg_simp, walkers, nsteps)\n",
    "acceptance2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now look at the samples that have been taken, the parameter values should begin to steady out after a certain number of steps called the burnin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examining the Gibbs sampler\n",
    "names = [\"M\", \"Omat\", \"pbad\", \"Y\", \"V\"]\n",
    "plots = []\n",
    "for (i,n) in enumerate(names)\n",
    "    p = plot(chain1[:,i], xlabel=\"MCMC step\", ylabel=n, label=\"\", tickfontsize=:5)\n",
    "    push!(plots, p)\n",
    "end\n",
    "plot(plots...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the parameter \"V\" begins to vary later on, the parameters we are trying ro fit, \"M\" and \"Omat\", steady out after 25000 or so steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examining the EMCEE sampler\n",
    "p1 = plot(chain2[:,:,1], legend=false, ylabel=\"M\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p2 = plot(chain2[:,:,2], legend=false, ylabel=\"Omat\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p3 = plot(chain2[:,:,3], legend=false, ylabel=\"Pbad\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p4 = plot(chain2[:,:,4], legend=false, ylabel=\"Y\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p5 = plot(chain2[:,:,5], legend=false, ylabel=\"V\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "plot(p1,p2,p3,p4,p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the plot M versus the MCMC step, it is clear that something is going wrong with some  of the walkers. Some of the walkers are settling at M < 15, when it is clear from the plot of the date that M must be around 20. Indeed, calculating the log probabilities for when M < 15 gives values less than -100, whereas calculating the log probabilities when M is approximately 20 gives values that are around -15. It is unclear what exactly is going wrong with ensemble_sampler, but we can eliminate the walkers that go astray using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of \"bad\" walkers\n",
    "count = 0\n",
    "goodWalk = zeros(nwalkers)\n",
    "for i in 1:nwalkers\n",
    "    if fgbg_SNLLL(z, m, merr, chain[end,i,1], chain[end,i,2], chain[end,i,3], chain[end,i,4], chain[end,i,5]) < -50\n",
    "        count = count + 1\n",
    "        goodWalk[i] = 0\n",
    "    else\n",
    "        goodWalk[i] = 1\n",
    "    end\n",
    "end\n",
    "\n",
    "# create a new array with only the \"good\" walkers\n",
    "chain_new = zeros(nsteps,nwalkers-count,nparams)\n",
    "j = 0\n",
    "for i in 1:nwalkers\n",
    "    if goodWalk[i] == 1\n",
    "        j = j+1\n",
    "        chain_new[:,j,:] = chain[:,i,:]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the improved result of the EMCEE sampler\n",
    "p1 = plot(chain_new[:,:,1], legend=false, ylabel=\"M\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p2 = plot(chain_new[:,:,2], legend=false, ylabel=\"Omat\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p3 = plot(chain_new[:,:,3], legend=false, ylabel=\"Pbad\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "p4 = plot(chain_new[:,:,4], legend=false, ylabel=\"Y\", xlabel=\"MCMC step\", tickfontsize=:7\n",
    "p5 = plot(chain_new[:,:,5], legend=false, ylabel=\"V\", xlabel=\"MCMC step\", tickfontsize=:7)\n",
    "plot(p1,p2,p3,p4,p5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plots look a bit better now, but the variance of the background \"V\" is still very eratic.\n",
    "\n",
    "However, we are mostly concerned with \"M\" and \"Omat\", so now we can take a look at the histograms of M versus Omat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin1 = 25000\n",
    "\n",
    "# Remove the burnin from the chain and plot the histogram for the Gibbs sampler\n",
    "\n",
    "MCMC_M = chain1[burnin1:end,1]\n",
    "MCMC_Omat = chain1[burnin1:end,2]\n",
    "histogram2d(MCMC_M, MCMC_Omat,xlabel=:\"M\",ylabel=:\"Omat\",\n",
    "    title=:\"Histogram of the values of M and Omat \\n found using the Gibbs sampler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burnin2 = 5_000\n",
    "\n",
    "# Remove the burnin from the chain, collapse the number of walkers to one (method copied from class notes)\n",
    "# and plot the histogram for the EMCEE sampler\n",
    "\n",
    "flat = chain_new[burnin2:end,:,:]\n",
    "nb,nw,np = size(flat)\n",
    "flat = reshape(flat, (nb*nw,np));\n",
    "\n",
    "histogram2d(flat[:,1], flat[:,2],xlabel=:\"M\",ylabel=:\"Omat\",\n",
    "    title=:\"Histogram of the values of M and Omat \\n found using the EMCEE sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms show the frequency with which each value of \"Omat\" and \"M\" were sampled. It is clear from the histograms that the samples of M and Omat have nice gaussian distributions using both the Gibbs and the EMCEE sampler.\n",
    "\n",
    "Now we can find the best fit for the functions by finding the position in each chain of samples that gives the highest value of fgbg_SNLLL, and we can plot those best fits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the parameters that maximize the log-likelihood for both the Gibbs and EMCEE samplers\n",
    "\n",
    "nsteps1 = length(chain1)\n",
    "\n",
    "prob = -100\n",
    "position1 = 0\n",
    "for n in burnin1:nsteps\n",
    "    # calculate the log-likelihood at step n\n",
    "    prob_new = fgbg_SNLLL(z, m, merr, chain1[n,1], chain1[n,2], chain1[n,3], chain1[n,4], chain1[n,5])\n",
    "    # If the new likelihood is greater than the old one, remember likelihood and the position where it occured\n",
    "    if prob_new > prob\n",
    "        prob = prob_new\n",
    "        position1 = n\n",
    "    end\n",
    "end\n",
    "\n",
    "nsteps2,npar2 = size(flat)\n",
    "\n",
    "prob = -100\n",
    "position2 = 0\n",
    "for n in 1:nsteps\n",
    "    # calculate the log-likelihood at step n\n",
    "    prob_new = fgbg_SNLLL(z, m, merr, flat[n,1], flat[n,2], flat[n,3], flat[n,4], flat[n,5])\n",
    "    if prob_new > prob\n",
    "        # If the new likelihood is greater than the old one, remember likelihood and the position where it occured\n",
    "        prob = prob_new\n",
    "        position2 = n\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data points\n",
    "plot(z, m, yerr=merr, seriestype=:scatter, label=\"\", xlabel=\"Redshift z\", ylabel=\"m_B\",legend=:bottomright,\n",
    "title=\"Perlmutter+99 Supernovae\")\n",
    "\n",
    "# Find the best M and Omat from the Gibbs Sampler\n",
    "M_bestG,Omat_bestG = chain1[position1,1], chain1[position1,2]\n",
    "\n",
    "# Calculate the function and plot it\n",
    "universe1 = LCDM(Omat_bestG,1-Omat_bestG, 60.)\n",
    "DM1 = map(z_new->distance_modulus(universe2,z_new),z_new)\n",
    "\n",
    "plot!(z_new,DM1 .+ M_meanG,color=:green, label=\"Best Fit with Gibbs Sampler\")\n",
    "\n",
    "# Find the best M and Omat from the Gibbs Sampler\n",
    "M_bestE,Omat_bestE = flat[position2,1], flat[position2,2]\n",
    "\n",
    "# Calculate the function and plot it\n",
    "universe2 = LCDM(Omat_bestE,1-Omat_bestE, 60.)\n",
    "DM2 = map(z_new->distance_modulus(universe,z_new),z_new)\n",
    "\n",
    "plot!(z_new,DM2 .+ M_bestE, color=:red, label=\"Best Fit with EMCEE Sampler\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the two best-fit curves are very similar, as predicted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    " The values of Omega_matter and their variances (which are the variances of the sampled Omat after the burnin) are given below, as are the \"best\" values of pbad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n \\n The value of Omega_matter using the Gibb's sampler is: \",chain1[position1,2], \" plus or minus \",\n",
    "    var(chain1[burnin1:end,2]),\n",
    "    \"\\n The value of Omega_matter using the EMCEE sampler is: \",flat[position2,2],\" plus or minus \",\n",
    "    var(flat[:,2]),\n",
    "    \"\\n \\n The value of pbad using the Gibb's sampler is: \",chain1[position1,3],\n",
    "    \"\\n The value of pbad using the EMCEE sampler is: \",flat[position2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the calculated values of Omega_matter are approximately the same to within statistical error, although they are bigger than the value obtained by Perlmutter et al, 0.28 plus or minus 1.\n",
    "\n",
    "The values of pbad tell us the percentage of points that safely disregarded, just multiply pbad by 100%. Appendix A contains two functions to find which points are outliers, but neither function has been checked for typos or properly commented due to time constraints, so they were not included in the body of the project.\n",
    "\n",
    "## Appendix A:\n",
    "\n",
    "Contains functions ptTest and worst, which can be used to determine which data points are outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function outputs the values of the foreground and background distributions at a each point\n",
    "\n",
    "function ptTest(x, y, sigma, M, Omat, pbad, Y, V)\n",
    "    # If the \"pbad\" parameter goes outside the range 0 to 1, the prior is zero, so bail out.\n",
    "    if (pbad < 0) || (pbad > 1)\n",
    "        return -Inf\n",
    "    end\n",
    "    # If the \"V\" parameter goes negative, the prior is zero; bail out.\n",
    "    if (V < 0 || V > 100)\n",
    "        return -Inf\n",
    "    end\n",
    "    if (Omat < 0 || Omat > 1)\n",
    "        return -Inf\n",
    "    end\n",
    "    if M < 0\n",
    "        return -Inf\n",
    "    end\n",
    "    \n",
    "    universe = LCDM(Omat ,1.0-Omat, 60.)\n",
    "    DM = map(x->distance_modulus(universe,x),x)\n",
    "\n",
    "    predM = M .+ DM\n",
    "    \n",
    "    # Note -- don't work in log space here, and do include the 1/(sqrt(2*pi)*sigma) term.\n",
    "    p_fg = @. 1. / (sqrt(2. *pi) * sigma) * exp(-0.5 * (predM - y)^2/(sigma^2))\n",
    "    bg_var = @. sigma^2 + V\n",
    "    p_bg = @. 1. / sqrt(2. *pi*bg_var) * exp(-0.5 * (y - Y)^2/bg_var)\n",
    "    # Here, we weight the foreground probability by 1-pbad and the background by pbad,\n",
    "    # and then take the log.\n",
    "    p_good = (1. .- pbad).*p_fg\n",
    "    p_bad = pbad.*p_bg\n",
    "    return p_good,p_bad\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good,bad = ptTest(z, m, merr, chain1[position1,1], chain1[position1,2], chain1[position1,3], chain1[position1,4],chain1[position1,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram(bad, xlabel=\"M\",bins=:10, label=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function uses the result of ptTest and the value of pBad to determine the outliers. It is very similar to \n",
    "#the reorder function\n",
    "\n",
    "function worst(b,percent)\n",
    "    n = length(b)\n",
    "    num_bad = Int(ceil(percent*n))\n",
    "    ai_temp = 0\n",
    "    bi_temp = 0\n",
    "    \n",
    "    a_new = zeros(n)\n",
    "    b_new = zeros(n)\n",
    "    \n",
    "    for p in 1:n\n",
    "        a_new[p] = p\n",
    "        b_new[p] = b[p]\n",
    "    end\n",
    "    \n",
    "    for i in 1:n\n",
    "        for j in 1:n\n",
    "            if b_new[i] < b_new[j]\n",
    "                ai_temp = a_new[i]\n",
    "                bi_temp = b_new[i]\n",
    "                a_new[i] = a_new[j]\n",
    "                b_new[i] = b_new[j]\n",
    "                a_new[j] = ai_temp\n",
    "                b_new[j] = bi_temp\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    bd = zeros(num_bad)\n",
    "    for i in 1:num_bad\n",
    "        bd[i] =  a_new[n+1-i]\n",
    "    end\n",
    "    \n",
    "    return bd\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "worst(bad,flat[position1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad[11],bad[17],bad[26],bad[22]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Works Cited\n",
    "\n",
    "S. Perlmutter et al \"Measurements of Omega and Lambda from high-redshift supernovae\" https://arxiv.org/abs/astro-ph/9812133\n",
    "\n",
    "D. Lang \"data-analysis-module\" https://github.com/eschnett/2018-computational-physics-course/tree/master/data-analysis-module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.0.0",
   "language": "julia",
   "name": "julia-1.0"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
